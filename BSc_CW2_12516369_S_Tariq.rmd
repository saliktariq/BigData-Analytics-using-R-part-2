---
title: "BSc Coursework 2"
author: "Salik Tariq / Student ID: 12516369"
output: pdf_document
---
# 1)	Bayesian Networks and Naïve Bayes Classifiers


### (a) Given a training dataset including 30 instances and a Bayesian network indicating the relationships between 3 features (i.e. Income, Student and Credit Rate), and the class attribute (i.e. Buy Computer), please create the conditional probability tables by hand.



### (b) Make predictions for 2 testing instances by using the Bayesian network classifier



### (c) Based on the conditional independence assumption between features, please create the conditional probability tables by hand.



### (d) Make predictions for 2 testing instances by using the naïve Bayes classifier



# 2) Decision Trees and Random Forests
## To predict room occupancy using the decision tree classification algorithm.

### (a) Load the room occupancy data and train a decision tree classifier. Evaluate the predictive performance by reporting the accuracy obtained on the testing dataset.
```{r}
#importing Libraries

library("rpart")
library("rpart.plot")
library("randomForest")
library("gplots")
library("ROCR")
library("pROC")

set.seed(300)
data_train <- read.csv(file="RoomOccupancy_Training.txt", header=TRUE, sep=",")
data_test <- read.csv(file="RoomOccupancy_Testing.txt", header=TRUE, sep=",")
#Exploring train DataSet
head(data_train)
#printing all columns with their data type
str(data_train)
#Checking Null values
any(is.na(data_train))

#Training Decision Tree Model
train_tree <-rpart(Occupancy ~.,method = "class",data = data_train)

#Evaluate the predictive performance

tree.preds <- predict(train_tree,data_test) 
print(head(tree.preds))

tree_pred <- as.data.frame(tree.preds)
prob <- function(a){
    if(a>=0.5){
        return('Yes')
    }else{
        return('No')
    }
}

tree_pred$Occupancy   <- sapply(tree_pred$Yes,prob)

print(head(tree_pred))

#reporting the accuracy obtained on the testing dataset

#confusion matrix
table_mat <- table(tree_pred$Occupancy,data_test$Occupancy)
print(table_mat)

accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test', accuracy_Test))



```

### (b) Output and analyse the tree learned by the decision tree algorithm, i.e. plot the tree structure and make a discussion about it.
```{r}
library("rpart")
library("rpart.plot")
library("randomForest")
d_tree <-rpart(Occupancy ~. , method = 'class' , data = data_train)
#Output and analyse and plotting a tree
rpart.plot(d_tree,uniform = T ,main = 'Occupancy Tree')

#interpretation of decision tree
rpart(formula = Occupancy ~ .,data = data_train, method = "class")

```

### (c) Train a random forests classifier, and evaluate the predictive performance by reporting the accuracy obtained on the testing dataset.
```{r}

rf_model <- randomForest(Occupancy ~., data = data_train , importance = TRUE)
print(rf_model)

#evaluate the predictive performance
#obtained on the testing dataset
rf_pred <- predict(rf_model,data_test)
rf_mat <- table(rf_pred,data_test$Occupancy)
print(rf_mat)

#reporting the accuracy
accuracy_Test <- sum(diag(rf_mat)) / sum(rf_mat)
print(paste('Accuracy for test', accuracy_Test))



```

### (d) Output and analyse the feature importance obtained by the random forests classifier.
```{r}

#Feature Importance
rf_model$importance


```


# 3) SVM
## To predict the wine quality using the support vector machine classification algorithm.

### (a) Download the wine quality data and use the training dataset to conduct the grid-search to find the optimal hyperparameters of svm by using the linear kernal.
```{r}
set.seed(300)
data_train <- read.csv(file="WineQuality_training.txt", header=TRUE, sep=",")
data_test <- read.csv(file="WineQuality_testing.txt", header=TRUE, sep=",")
#Exploring train DataSet
head(data_train)
#printing all columns with their data type
str(data_train)
#Checking Null values
any(is.na(data_train))

library("e1071")
model <- svm(quality ~., data = data_train,kernel = 'linear')
summary(model)

#Grid Search using Linear Kernel and finding optimal hyperparameter
h_tune <- tune(svm,train.x = data_train[1:11] , train.y = data_train[,12] ,
               kernel = 'linear'  ,
               ranges = list(cost = c(0.01, 0.1, 1, 5, 10), gamma = c(0.01, 0.03, 0.1, 0.5, 1)))
summary(h_tune)
```

### (b) Train a svm classifier by using the linear kernal and the corresponding optimal hyperparameters, then make predictions on the testing dataset, report the predictive performance.

model_lin <- svm(quality ~., data = data_train,kernel = 'linear' 
             , cost =10 , gamma = 0.01 )

#make predictions on the testing dataset, 
#report the predictive performance

model_pred <- predict(model,data_test)

#reporting the accuracy
svm_mat <- table(model_pred,data_test$quality)
print(svm_mat)
accuracy_Test <- sum(diag(svm_mat)) / sum(svm_mat)
print(paste('Accuracy for test', accuracy_Test))


```

### (c) Conduct the grid-search to find the optimal hyperparameters of svm by using the RBF kernal.

```{r}
#Training SVM using RBF
model <- svm(quality ~., data = data_train,  kernel = 'radial')

#Grid Search using RBF Kernel and finding optimal hyperparameter
hrbf_tune <- tune(svm,train.x = data_train[1:11] , train.y = data_train[,12] ,
               kernel = 'radial'  ,
               ranges = list(cost = c(0.01, 0.1, 1, 5, 10), gamma = c(0.01, 0.03, 0.1, 0.5, 1)))
summary(hrbf_tune)

model_rbf <- svm(quality ~., data = data_train,kernel = 'radial' 
             , cost =5 , gamma = 0.5 ,decision.values = TRUE,probability = TRUE)

model_predrbf <- predict(model_rbf,data_test)

print(model_predrbf)
summary(model_predrbf)

```


### (d) Train a svm classifier by using the RBF kernal and the corresponding optimal hyperparameters, then make predictions on the testing dataset, report the predictive performance.

```{r}
svm.rbf_mat <- table(model_predrbf,data_test$quality)
print(svm.rbf_mat)
accuracy_Test <- sum(diag(svm.rbf_mat)) / sum(svm.rbf_mat)
print(paste('Accuracy for test', accuracy_Test))


```

### (e) Conduct the ROC curve analysis to compare the predictive performance of svm classifiers trained by using the linear and RBF kernels respectively.
#### Hint: Given a pre-defined hyperparameter space - C: [0.01, 0.1, 1, 5, 10], and γ: [0.01, 0.03, 0.1, 0.5, 1].

```{r}
library("ROSE")
rbf_pred <- predict(model_rbf,data_test)
lin_pred <- predict(model_lin,data_test)

rbf_pred <- prediction(as.numeric(rbf_pred) , as.numeric(data_test$quality))
roc <- performance(rbf_pred,'tpr','fpr')
plot(roc , main = "ROC for RBF Kernel",col = 'red')
abline(a=0, b=1,col = 'blue')


lin_pred <- prediction(as.numeric(lin_pred) , as.numeric(data_test$quality))
roc <- performance(lin_pred,'tpr','fpr')
plot(roc , main = "ROC for Linear Kernel",col = 'red')
abline(a=0, b=1,col = 'blue')



```

# 4) Hierarchical Clustering
## Consider the USArrests data. We will now perform hierarchical clustering on the states.


### (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
data("USArrests")
data_sets <- USArrests 

data_euclidean <-dist(data_sets, method = "euclidean")

#hierarchical clustering
euclidean_cluster <- hclust(data_euclidean, method="complete")

#cluster the states
plot(euclidean_cluster, main="hierarchical clustering with complete linkage and Euclidean distance")


```




### (b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}
clust_avg <- hclust(data, method = 'average')
plot(clust_avg)
cut_avg <- cutree(clust_avg, k = 3)
plot(clust_avg)
rect.hclust(clust_avg , k = 3, border = 2:6)
abline(h = 3, col = 'blue')
```


### (c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

```{r}
data_sets <- as.data.frame(scale(data_sets))

data<-dist(data_sets, method = 'euclidean')
#hierarchical clustering
hiera_data<-hclust(data, method="complete")
#cluster the states
plot(hiera_data)

```

### (d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

In scaling we are transforming numerical values to get specific helpful properties In scaling we are changing 
the range of data as we can see earlier the range/height  was 0-300 and after scaling it shrinks to 0-6.We should have 
scaled the data before because whenever we have  parameters/Features that differ from each other in terms of range of values 
then you have to normalise the data so that the difference in these range of values does not affect your outcome.

```{r}
rm(list=ls())
```